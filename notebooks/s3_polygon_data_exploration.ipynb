{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6fc3e72-ea3e-4212-b7cb-45a51aaf1007",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3 #Amazon AWS Python SDK\n",
    "from botocore.config import Config #Config for SDK\n",
    "from dotenv import load_dotenv # Load .ENV file containing protected information\n",
    "import os # Ability to manage and access neigboring files \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8eb67cc-ad0d-4c5f-a2d7-8a340f882fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the environment variables available to python from the .env file\n",
    "load_dotenv()\n",
    "# Load the environment variables into python variables\n",
    "ACCESS_KEY = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "SECRET_KEY = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "POLYGON_KEY = os.getenv(\"POLYGON_API_KEYS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b2e2e56-7aff-4b0c-a5fd-811223183779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a session using the AWS keys\n",
    "session = boto3.Session( # Session object used to configure users and environment control\n",
    "    aws_access_key_id=ACCESS_KEY,\n",
    "    aws_secret_access_key=SECRET_KEY,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a1cc006-48ed-437a-b387-e6207ec2c7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a client with session and speficy the endpoint (where the data is located)\n",
    "s3 = session.client(\n",
    "    's3', # Connecting to the S3 (Simple Storage Service) specifically (can connect to any aws service here)\n",
    "    endpoint_url='https://files.polygon.io', # Base url for the service you want to access\n",
    "    config=Config(signature_version='s3v4'), # Ensures client is using AWS signature Version 4 protocol by prohibiting api requests unless supplied with\n",
    "                                             # a secret key. Used for hashsing\n",
    ")\n",
    "# The previous code is everything needed to accesss the S3 flatfiles, from here you can use commands like list objects or get objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32059842-ab37-482b-9da9-198a82bebde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a paginator for listing objects\n",
    "paginator = s3.get_paginator('list_objects_v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d92d8a-8d2b-43e2-94fd-c4e8f14fc85e",
   "metadata": {},
   "source": [
    "## üåê Understanding Requests and Paginators in S3 (Conceptual Overview)\n",
    "\n",
    "### üì§ What is a Request?\n",
    "\n",
    "A **request** is a single operation sent from your client (e.g., Python code) to a server (e.g., AWS S3 or Polygon‚Äôs S3-compatible endpoint). For example, when you ask to list files in a folder-like structure in a bucket, that is a request.\n",
    "\n",
    "S3‚Äôs `list_objects_v2` request returns a maximum of 1000 objects (files) at a time. If more files exist, it only returns the first \"page\" and indicates that more data is available.\n",
    "\n",
    "---\n",
    "\n",
    "### üîÅ What is a Paginator?\n",
    "\n",
    "A **paginator** is a built-in tool provided by `boto3` that automatically handles repeated requests when the response is paginated. \n",
    "\n",
    "Instead of manually tracking continuation tokens and sending new requests, the paginator transparently performs this for you. It lets you iterate over all the data as if it were returned in one big response.\n",
    "\n",
    "---\n",
    "\n",
    "### ü™£ S3 Paginators Specifically\n",
    "\n",
    "S3 paginators are used to retrieve more than 1000 files (objects) from a bucket. You create a paginator specifically for the `list_objects_v2` operation, which is the improved version of the original S3 listing API.\n",
    "\n",
    "The paginator handles:\n",
    "- Sending the first request\n",
    "- Detecting if the result is truncated (chopped off)\n",
    "- Sending follow-up requests with the continuation token\n",
    "- Returning each full page of results one after the other\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Key Parameters Used with S3 Paginators\n",
    "\n",
    "- **Bucket**: The name of the S3 bucket you are querying.\n",
    "- **Prefix**: A folder-like path that limits the results to objects that begin with that string.\n",
    "- **Delimiter** (optional): Used to group files as if they were in folders (commonly set to `/`).\n",
    "- **PaginationConfig** (optional): Allows advanced control, like page size or starting from a specific point.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Summary\n",
    "\n",
    "- A **request** retrieves a single chunk of data from S3.\n",
    "- A **paginator** automates multiple requests so you can work with large datasets easily.\n",
    "- S3 paginators are essential when listing more than 1000 files in a bucket or folder-like structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32c6eb49-11d7-47bb-b084-4217e0f49da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from polygon import RESTClient\n",
    "from polygon.rest.models import (\n",
    "    TickerSnapshot,\n",
    "    Agg,\n",
    ") #Python libraries for polygon\n",
    "\n",
    "client = RESTClient(POLYGON_KEY) # Activating Polygon REST API\n",
    "\n",
    "snapshot = client.get_snapshot_all(\n",
    "\t\"stocks\",\n",
    "\t) # Returns a snapshot of all stocks daily activities, includes over 10000 tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47a87b6d-38e7-4336-a2de-b01200e2300a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(snapshot) # Make json object a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27c73171-f76a-4c2c-bfaa-41c627450435",
   "metadata": {},
   "outputs": [],
   "source": [
    "day_df = pd.json_normalize(df['day']) # Turn json dict into dataframe\n",
    "day_df = day_df[['volume', 'vwap']]  # Only keep the ones you care about (Volume and VWAP)\n",
    "final_df = pd.concat([df['ticker'], day_df], axis=1) # Concatenate the ticker onto the rows on the left side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "210f04d7-ef97-4403-b6b0-d4b318fa143f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['dollar_volume'] = df['day'].apply(lambda x: x['volume'] * x['vwap']) # Lambda function to create dollar volume\n",
    "final_df = final_df.sort_values(by='dollar_volume', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12ab0bc2-14a8-4c45-ba0e-27406728e8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "ticker_list = list(final_df['ticker'][:500])\n",
    "print(len(ticker_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79d7b012-3172-4fec-89bc-6cef550859ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ticker           0\n",
       "volume           0\n",
       "vwap             0\n",
       "dollar_volume    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.isnull().sum() # Print null values per column to check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e98c3fc-4cb9-4963-8599-a44feb6bbd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy example\n",
    "# Specify the bucket name\n",
    "bucket_name = 'flatfiles'\n",
    "\n",
    "# Specify the S3 object key name\n",
    "object_key = 'us_stocks_sip/minute_aggs_v1/2023/03/2023-03-21.csv.gz'\n",
    "\n",
    "# Specify the local file name and path to save the downloaded file\n",
    "# This splits the object_key string by '/' and takes the last segment as the file name\n",
    "local_file_name = object_key.split('/')[-1]\n",
    "\n",
    "# This constructs the full local file path\n",
    "local_file_path = './' + local_file_name\n",
    "\n",
    "# Download the file\n",
    "s3.download_file(bucket_name, object_key, local_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bb655b2-8915-48d5-849e-60741d5013ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>volume</th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>window_start</th>\n",
       "      <th>transactions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>11905</td>\n",
       "      <td>121.720</td>\n",
       "      <td>122.055</td>\n",
       "      <td>122.055</td>\n",
       "      <td>121.720</td>\n",
       "      <td>1742477400000000000</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>1268</td>\n",
       "      <td>122.075</td>\n",
       "      <td>122.075</td>\n",
       "      <td>122.075</td>\n",
       "      <td>122.075</td>\n",
       "      <td>1742477460000000000</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>565</td>\n",
       "      <td>122.075</td>\n",
       "      <td>122.075</td>\n",
       "      <td>122.075</td>\n",
       "      <td>122.075</td>\n",
       "      <td>1742477520000000000</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>8163</td>\n",
       "      <td>122.075</td>\n",
       "      <td>122.070</td>\n",
       "      <td>122.080</td>\n",
       "      <td>121.440</td>\n",
       "      <td>1742477580000000000</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>4209</td>\n",
       "      <td>122.065</td>\n",
       "      <td>120.810</td>\n",
       "      <td>122.065</td>\n",
       "      <td>120.810</td>\n",
       "      <td>1742477640000000000</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker  volume     open    close     high      low         window_start  \\\n",
       "0      A   11905  121.720  122.055  122.055  121.720  1742477400000000000   \n",
       "1      A    1268  122.075  122.075  122.075  122.075  1742477460000000000   \n",
       "2      A     565  122.075  122.075  122.075  122.075  1742477520000000000   \n",
       "3      A    8163  122.075  122.070  122.080  121.440  1742477580000000000   \n",
       "4      A    4209  122.065  120.810  122.065  120.810  1742477640000000000   \n",
       "\n",
       "   transactions  \n",
       "0            36  \n",
       "1            17  \n",
       "2            16  \n",
       "3            81  \n",
       "4           117  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming you've already downloaded the file\n",
    "df = pd.read_csv(local_file_path, compression='gzip')\n",
    "\n",
    "# Preview the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1420216-63a5-4d76-9472-6d62310f1eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "498\n"
     ]
    }
   ],
   "source": [
    "#Now I will create a dataframe where the tickers only match the ones in the ticker list, we will filter them out and check if the \n",
    "# minute agg bars amount is the same across tickers. \n",
    "\n",
    "filtered_df = df[df['ticker'].isin(ticker_list)]\n",
    "print(len(filtered_df['ticker'].unique()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "822f0f2f-6cfb-4b6c-8a33-05c699dc38d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing tickers:\n",
      "CRWV\n",
      "NMAX\n"
     ]
    }
   ],
   "source": [
    "# Now I will print the missing tickers that did not get imported but was in the ticker list\n",
    "\n",
    "filtered_tickers = set(filtered_df['ticker'].unique())\n",
    "expected_tickers = set(ticker_list)\n",
    "\n",
    "missing = expected_tickers - filtered_tickers\n",
    "\n",
    "print(\"Missing tickers:\")\n",
    "for ticker in sorted(missing):\n",
    "    print(ticker)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2af34602-fb69-4a34-9d95-6a26c17ad53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_minutes(df, timestamp_col='window_start', ticker_col='ticker'):\n",
    "     # Convert nanosecond timestamps to datetime in UTC\n",
    "    df[timestamp_col] = pd.to_datetime(df[timestamp_col], unit='ns', utc=True)\n",
    "\n",
    "    # Filter to regular market hours in UTC (13:30 to 20:00)\n",
    "    df = df[\n",
    "        (df[timestamp_col].dt.time >= pd.to_datetime(\"13:30\").time()) &\n",
    "        (df[timestamp_col].dt.time < pd.to_datetime(\"20:00\").time())\n",
    "    ]\n",
    "\n",
    "    # Count bars per ticker\n",
    "    bar_counts = df.groupby(ticker_col).size()\n",
    "\n",
    "    # Print diagnostic output\n",
    "    if bar_counts.nunique() == 1:\n",
    "        print(f\"All tickers have {bar_counts.iloc[0]} minute bars during regular market hours.\")\n",
    "    else:\n",
    "        print(\" Tickers have different numbers of regular market bars.\")\n",
    "        print(bar_counts.sort_values())\n",
    "\n",
    "    return bar_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a4bf2254-e083-42eb-8d0c-4e2096e661dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Tickers have different numbers of regular market bars.\n",
      "ticker\n",
      "AZO      44\n",
      "ICCT     68\n",
      "TDG      97\n",
      "BKNG    161\n",
      "ARGX    165\n",
      "       ... \n",
      "XLB     390\n",
      "ABBV    390\n",
      "AAPL    390\n",
      "AAL     390\n",
      "XBI     390\n",
      "Length: 498, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_106886/2811558647.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[timestamp_col] = pd.to_datetime(df[timestamp_col], unit='ns', utc=True)\n"
     ]
    }
   ],
   "source": [
    "bar_counts = check_minutes(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "458026a8-7b19-4f0e-a89d-940f4a4be2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240 tickers have exactly 390 minute bars.\n"
     ]
    }
   ],
   "source": [
    "num_with_390 = (bar_counts == 390).sum()\n",
    "print(f\"{num_with_390} tickers have exactly 390 minute bars.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "84dd6faf-1708-44bb-9c94-0fb91a573312",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_with_390 = bar_counts[bar_counts == 390].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cb819076-e878-4e83-9c0a-1e6d5f2d2c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from functools import reduce\n",
    "\n",
    "def verify_consistent_390_minute_tickers_with_filter(file_paths, ticker_list):\n",
    "    \"\"\"\n",
    "    Verifies that each file contains only the specified tickers and that each has\n",
    "    exactly 390 regular market bars for every trading day.\n",
    "\n",
    "    Parameters:\n",
    "        file_paths (List[str]): List of .csv.gz file paths (one per trading day).\n",
    "        ticker_list (List[str]): List of tickers to keep/check in each file.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if all tickers in ticker_list have exactly 390 bars across all files.\n",
    "    \"\"\"\n",
    "    expected_ticker_count = len(ticker_list)\n",
    "    ticker_set = set(ticker_list)\n",
    "    daily_valid_ticker_sets = []\n",
    "\n",
    "    for path in file_paths:\n",
    "        print(f\"\\n Processing {path}...\")\n",
    "\n",
    "        # Load the file\n",
    "        df = pd.read_csv(path, compression='gzip')\n",
    "\n",
    "        # Filter to only tickers in the provided list\n",
    "        df = df[df['ticker'].isin(ticker_list)]\n",
    "\n",
    "        # Run your existing minute bar checker\n",
    "        bar_counts = check_minutes(df)\n",
    "\n",
    "        # Get tickers with exactly 390 bars\n",
    "        valid_tickers = bar_counts[bar_counts == 390].index.tolist()\n",
    "\n",
    "        # Check that all expected tickers are present\n",
    "        missing_tickers = ticker_set - set(valid_tickers)\n",
    "        if len(valid_tickers) != expected_ticker_count:\n",
    "            print(f\"{len(valid_tickers)} tickers have 390 bars (expected {expected_ticker_count})\")\n",
    "            print(f\" Missing or incomplete tickers: {sorted(missing_tickers)}\")\n",
    "\n",
    "        daily_valid_ticker_sets.append(set(valid_tickers))\n",
    "\n",
    "    # Check consistency across all files\n",
    "    common_tickers = reduce(set.intersection, daily_valid_ticker_sets)\n",
    "    if len(common_tickers) != expected_ticker_count:\n",
    "        print(f\"\\nOnly {len(common_tickers)} tickers had full 390 bars every day.\")\n",
    "        return False\n",
    "\n",
    "    print(f\"\\n All {expected_ticker_count} tickers had 390 bars in every file.\")\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0c032ab3-40aa-4356-a2ef-89e1b0e9f52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÇ Processing ./2023-03-20.csv.gz...\n",
      "‚ùå Tickers have different numbers of regular market bars.\n",
      "ticker\n",
      "IBIT      1\n",
      "NVDL     91\n",
      "AZO     108\n",
      "JAAA    170\n",
      "ARGX    175\n",
      "       ... \n",
      "BA      390\n",
      "BABA    390\n",
      "BAC     390\n",
      "BBY     390\n",
      "AAL     390\n",
      "Length: 483, dtype: int64\n",
      "‚ùå 273 tickers have 390 bars (expected 500)\n",
      "‚ö†Ô∏è  Missing or incomplete tickers: ['A', 'ACWI', 'ADI', 'ADP', 'ADSK', 'AJG', 'AMGN', 'AON', 'APP', 'ARES', 'ARGX', 'ARM', 'ASML', 'AWK', 'AZO', 'BDX', 'BECN', 'BJ', 'BKLN', 'BKNG', 'BLK', 'BRK.A', 'BURL', 'CCI', 'CDNS', 'CDW', 'CEG', 'CHTR', 'CI', 'CLS', 'CME', 'CMG', 'CMI', 'COHR', 'COR', 'COST', 'CRH', 'CROX', 'CRWV', 'CTAS', 'DE', 'DECK', 'DG', 'DHR', 'DKS', 'DLR', 'DLTR', 'DRI', 'DUK', 'EA', 'ECL', 'ED', 'EL', 'ELV', 'ENTG', 'EQIX', 'ETR', 'EWJ', 'EWW', 'EXE', 'EXPD', 'EXPE', 'FERG', 'FI', 'FIVE', 'FLUT', 'FND', 'FSLR', 'GAP', 'GEHC', 'GEV', 'GLDM', 'GRMN', 'HCA', 'HDB', 'HES', 'HIMS', 'HLT', 'HOOD', 'HSY', 'HUBS', 'HUM', 'IAU', 'IBIT', 'ICCT', 'IDXX', 'IGSB', 'IGV', 'IJH', 'INDA', 'INTU', 'IONQ', 'ISRG', 'ITOT', 'IVW', 'IWB', 'IWD', 'IWF', 'JAAA', 'JEPQ', 'KLAC', 'KVUE', 'LEN', 'LIN', 'LLY', 'LMT', 'LNG', 'LPLA', 'LRCX', 'LULU', 'LVS', 'LW', 'LYB', 'LYV', 'MCK', 'MCO', 'MDB', 'MDY', 'MELI', 'MINT', 'MMC', 'MNST', 'MOH', 'MPWR', 'MSI', 'MSTR', 'MSTU', 'MSTZ', 'MUB', 'NMAX', 'NOC', 'NOW', 'NSC', 'NU', 'NUE', 'NVDL', 'NVO', 'NXPI', 'ODFL', 'ONON', 'ORLY', 'OTIS', 'PANW', 'PAYX', 'PGR', 'PH', 'PLD', 'PSLV', 'PWR', 'QQQM', 'RDDT', 'REGN', 'RH', 'RL', 'ROP', 'ROST', 'RSG', 'SAP', 'SCHG', 'SCHX', 'SGI', 'SGOV', 'SH', 'SHV', 'SHW', 'SHY', 'SHYG', 'SKX', 'SMCI', 'SN', 'SNPS', 'SOXX', 'SPG', 'SPGI', 'SPIB', 'SPLG', 'SPOT', 'SRE', 'SRLN', 'STZ', 'SWK', 'SYK', 'TDG', 'TEAM', 'TIP', 'TMO', 'TRGP', 'TRV', 'TSCO', 'TSLQ', 'TT', 'TTWO', 'ULTA', 'UNH', 'UPS', 'URI', 'USFR', 'USHY', 'UVIX', 'VB', 'VCLT', 'VCSH', 'VGT', 'VIG', 'VMC', 'VO', 'VRSK', 'VRT', 'VRTX', 'VT', 'VTEB', 'VTIP', 'VTV', 'VTWO', 'WCN', 'WDAY', 'WEC', 'WELL', 'WING', 'WM', 'WSM', 'WYNN', 'XLRE', 'XYZ', 'YUM', 'ZS', 'ZTS']\n",
      "\n",
      "üìÇ Processing ./2023-03-21.csv.gz...\n",
      "‚ùå Tickers have different numbers of regular market bars.\n",
      "ticker\n",
      "IBIT      3\n",
      "AZO      83\n",
      "JAAA    117\n",
      "NVDL    130\n",
      "CMG     169\n",
      "       ... \n",
      "BA      390\n",
      "BABA    390\n",
      "BAC     390\n",
      "BBY     390\n",
      "AAL     390\n",
      "Length: 483, dtype: int64\n",
      "‚ùå 260 tickers have 390 bars (expected 500)\n",
      "‚ö†Ô∏è  Missing or incomplete tickers: ['A', 'ACWI', 'ADP', 'ADSK', 'AJG', 'AMGN', 'AON', 'APP', 'ARES', 'ARGX', 'ARM', 'ASML', 'AVGO', 'AWK', 'AZN', 'AZO', 'BDX', 'BECN', 'BIDU', 'BIL', 'BJ', 'BKLN', 'BKNG', 'BLK', 'BRK.A', 'BURL', 'CAH', 'CB', 'CDNS', 'CDW', 'CEG', 'CHTR', 'CI', 'CLS', 'CME', 'CMG', 'CMI', 'CNC', 'COHR', 'COR', 'COST', 'CRH', 'CROX', 'CRWV', 'CTAS', 'DDOG', 'DE', 'DECK', 'DG', 'DHR', 'DIA', 'DKS', 'DLTR', 'DRI', 'EA', 'ECL', 'EL', 'ELV', 'EMB', 'ENTG', 'EQIX', 'ETN', 'ETR', 'EWW', 'EXE', 'EXPD', 'EXPE', 'FERG', 'FI', 'FIVE', 'FLUT', 'FND', 'FSLR', 'GAP', 'GEHC', 'GEV', 'GLDM', 'GLW', 'GRMN', 'GS', 'GSK', 'HCA', 'HES', 'HIMS', 'HLT', 'HSY', 'HUBS', 'HUM', 'IBIT', 'ICCT', 'ICE', 'IDXX', 'IGSB', 'IGV', 'IJH', 'INDA', 'INTU', 'IONQ', 'ISRG', 'ITOT', 'IVW', 'IWB', 'IWD', 'IWF', 'JAAA', 'JEPQ', 'JNK', 'KKR', 'KLAC', 'KVUE', 'KWEB', 'LEN', 'LIN', 'LMT', 'LNG', 'LOW', 'LPLA', 'LRCX', 'LULU', 'LW', 'LYB', 'LYV', 'MA', 'MAR', 'MCD', 'MCK', 'MCO', 'MDB', 'MDY', 'MELI', 'MINT', 'MMC', 'MNST', 'MOH', 'MPWR', 'MSI', 'MSTR', 'MSTU', 'MSTZ', 'MUB', 'NET', 'NMAX', 'NOC', 'NOW', 'NSC', 'NU', 'NUE', 'NVDL', 'NVO', 'NXPI', 'ODFL', 'ORLY', 'OTIS', 'PAYX', 'PFF', 'PGR', 'PH', 'PSLV', 'PWR', 'QQQM', 'QUAL', 'RDDT', 'REGN', 'RH', 'RL', 'ROP', 'ROST', 'RSG', 'RSP', 'SAP', 'SCHG', 'SCHX', 'SGI', 'SGOV', 'SHV', 'SHW', 'SHY', 'SHYG', 'SKX', 'SMCI', 'SN', 'SNPS', 'SOXX', 'SPGI', 'SPIB', 'SPLG', 'SPOT', 'SRLN', 'STX', 'STZ', 'SVIX', 'SWK', 'SYK', 'TDG', 'TEAM', 'TIP', 'TMO', 'TPR', 'TRGP', 'TRV', 'TSCO', 'TSLQ', 'TT', 'TTWO', 'UL', 'ULTA', 'UNH', 'URI', 'USFR', 'USHY', 'USO', 'UVIX', 'VB', 'VCLT', 'VGT', 'VIG', 'VMC', 'VO', 'VRSK', 'VRT', 'VRTX', 'VT', 'VTEB', 'VTI', 'VTIP', 'VTV', 'VTWO', 'VUG', 'W', 'WCN', 'WDAY', 'WEC', 'WING', 'WM', 'WSM', 'WYNN', 'XHB', 'XYZ', 'YUM', 'ZTS']\n",
      "\n",
      "üìÇ Processing ./2024-03-20.csv.gz...\n",
      "‚ùå Tickers have different numbers of regular market bars.\n",
      "ticker\n",
      "ICCT     41\n",
      "FLUT    137\n",
      "CTAS    167\n",
      "TDG     170\n",
      "AZO     193\n",
      "       ... \n",
      "XRT     390\n",
      "ABNB    390\n",
      "ABBV    390\n",
      "AAPL    390\n",
      "AAL     390\n",
      "Length: 490, dtype: int64\n",
      "‚ùå 203 tickers have 390 bars (expected 500)\n",
      "‚ö†Ô∏è  Missing or incomplete tickers: ['A', 'ABT', 'ACWI', 'ADP', 'ADSK', 'AEM', 'AGG', 'AJG', 'ALL', 'AMGN', 'AMT', 'ANET', 'AON', 'APH', 'APO', 'ARES', 'ARGX', 'ASML', 'AWK', 'AXP', 'AZN', 'AZO', 'BDX', 'BECN', 'BIDU', 'BJ', 'BK', 'BKNG', 'BLK', 'BRK.A', 'BURL', 'BX', 'CAH', 'CAT', 'CB', 'CCI', 'CDNS', 'CDW', 'CEG', 'CHTR', 'CI', 'CME', 'CMG', 'CMI', 'CNC', 'COF', 'COHR', 'COR', 'COST', 'CP', 'CRH', 'CROX', 'CRWD', 'CRWV', 'CTAS', 'CVS', 'DASH', 'DDOG', 'DE', 'DECK', 'DFS', 'DG', 'DHI', 'DHR', 'DKS', 'DLR', 'DLTR', 'DRI', 'DUK', 'EA', 'ECL', 'ED', 'EEM', 'EFA', 'EL', 'ELV', 'EMB', 'ENTG', 'EOG', 'EQIX', 'ET', 'ETN', 'ETR', 'EW', 'EWJ', 'EWW', 'EWZ', 'EXE', 'EXPD', 'EXPE', 'FANG', 'FDX', 'FERG', 'FI', 'FIVE', 'FLUT', 'FND', 'FSLR', 'GAP', 'GEV', 'GLDM', 'GLW', 'GRMN', 'GS', 'HCA', 'HDB', 'HES', 'HLT', 'HON', 'HSY', 'HUBS', 'HUM', 'IAU', 'ICCT', 'ICE', 'IDXX', 'IEF', 'IEMG', 'IGSB', 'INDA', 'INTU', 'ISRG', 'ITOT', 'IVW', 'IWB', 'IWD', 'IWF', 'JAAA', 'JCI', 'JEPI', 'JEPQ', 'JNK', 'KBWB', 'KKR', 'KLAC', 'LCID', 'LEN', 'LIN', 'LLY', 'LMT', 'LNG', 'LOW', 'LPLA', 'LQD', 'LRCX', 'LULU', 'LVS', 'LW', 'LYB', 'LYV', 'MA', 'MAR', 'MCD', 'MCHP', 'MCK', 'MCO', 'MDB', 'MDY', 'MELI', 'MET', 'MINT', 'MMC', 'MOH', 'MPC', 'MPWR', 'MSI', 'MSTU', 'MSTZ', 'MUB', 'NET', 'NFLX', 'NMAX', 'NOC', 'NOW', 'NSC', 'NUE', 'NXPI', 'ODFL', 'OKE', 'ORLY', 'OTIS', 'PAYX', 'PCAR', 'PFF', 'PGR', 'PH', 'PINS', 'PLD', 'PNC', 'PSLV', 'PWR', 'QLD', 'QQQM', 'QUAL', 'RCL', 'RDDT', 'REGN', 'RH', 'RL', 'ROKU', 'ROP', 'ROST', 'RSG', 'RSP', 'SAP', 'SCHG', 'SCHX', 'SE', 'SGI', 'SGOV', 'SH', 'SHEL', 'SHV', 'SHW', 'SHY', 'SHYG', 'SKX', 'SLV', 'SN', 'SNPS', 'SPG', 'SPGI', 'SPIB', 'SPLG', 'SPOT', 'SPXS', 'SPXU', 'SRE', 'SRLN', 'SSO', 'STX', 'STZ', 'SVIX', 'SVXY', 'SWK', 'SYF', 'SYK', 'TDG', 'TEAM', 'TGT', 'TIP', 'TMF', 'TMO', 'TMUS', 'TRGP', 'TRV', 'TSCO', 'TSLQ', 'TT', 'TTWO', 'TWLO', 'TZA', 'UL', 'ULTA', 'UNP', 'UPRO', 'UPST', 'URI', 'USFR', 'USHY', 'USMV', 'UVIX', 'V', 'VB', 'VCLT', 'VCSH', 'VGT', 'VIG', 'VMC', 'VO', 'VRSK', 'VRTX', 'VT', 'VTEB', 'VTIP', 'VTR', 'VTV', 'VTWO', 'VUG', 'VXX', 'W', 'WCN', 'WDAY', 'WEC', 'WELL', 'WING', 'WM', 'WSM', 'WYNN', 'XLB', 'XLC', 'XLRE', 'XOP', 'XYZ', 'YUM', 'ZS', 'ZTS']\n",
      "\n",
      "üìÇ Processing ./2025-03-20.csv.gz...\n",
      "‚ùå Tickers have different numbers of regular market bars.\n",
      "ticker\n",
      "AZO      44\n",
      "ICCT     68\n",
      "TDG      97\n",
      "BKNG    161\n",
      "ARGX    165\n",
      "       ... \n",
      "XLB     390\n",
      "ABBV    390\n",
      "AAPL    390\n",
      "AAL     390\n",
      "XBI     390\n",
      "Length: 498, dtype: int64\n",
      "‚ùå 240 tickers have 390 bars (expected 500)\n",
      "‚ö†Ô∏è  Missing or incomplete tickers: ['A', 'ABNB', 'ABT', 'ADI', 'ADP', 'ADSK', 'AEM', 'AEP', 'AIG', 'AJG', 'ALL', 'AMGN', 'AMT', 'AON', 'ARES', 'ARGX', 'ARM', 'ASML', 'AWK', 'AXP', 'AZO', 'BDX', 'BJ', 'BKLN', 'BKNG', 'BLK', 'BND', 'BRK.A', 'BURL', 'BX', 'CAH', 'CAT', 'CB', 'CCI', 'CDNS', 'CDW', 'CEG', 'CHTR', 'CI', 'CME', 'CMI', 'CNC', 'COHR', 'COR', 'COST', 'CP', 'CROX', 'CRWV', 'CTAS', 'DDOG', 'DE', 'DECK', 'DELL', 'DFS', 'DG', 'DHR', 'DIA', 'DKS', 'DLR', 'DLTR', 'DRI', 'EBAY', 'ECL', 'ED', 'EL', 'ELV', 'EMB', 'ENTG', 'EQIX', 'ETN', 'ETR', 'EW', 'EWW', 'EXPD', 'EXPE', 'FANG', 'FAST', 'FDX', 'FERG', 'FI', 'FLUT', 'FND', 'FSLR', 'GEHC', 'GLDM', 'GRMN', 'GSK', 'HCA', 'HD', 'HDB', 'HES', 'HLT', 'HSY', 'HUBS', 'HUM', 'IAU', 'ICCT', 'ICE', 'IDXX', 'IEF', 'IGSB', 'INDA', 'INTU', 'IP', 'ISRG', 'ITOT', 'IWB', 'IWD', 'IWF', 'IYR', 'JAAA', 'JNK', 'KBWB', 'KLAC', 'LIN', 'LLY', 'LMT', 'LNG', 'LOW', 'LPLA', 'LULU', 'LVS', 'LW', 'LYB', 'LYV', 'MA', 'MAR', 'MCD', 'MCK', 'MCO', 'MDB', 'MDY', 'MELI', 'MET', 'MINT', 'MMC', 'MOH', 'MPC', 'MPWR', 'MRK', 'MS', 'MSI', 'MUB', 'NET', 'NFLX', 'NMAX', 'NOC', 'NOW', 'NSC', 'NUE', 'NXPI', 'ODFL', 'OKE', 'ON', 'ORLY', 'OTIS', 'PAYX', 'PCAR', 'PFF', 'PGR', 'PH', 'PLD', 'PNC', 'PSLV', 'PSX', 'PWR', 'QLD', 'QQQM', 'QUAL', 'RCL', 'REGN', 'RH', 'RL', 'ROP', 'ROST', 'RSG', 'SAP', 'SCHX', 'SGI', 'SH', 'SHV', 'SHW', 'SHY', 'SHYG', 'SKX', 'SN', 'SNPS', 'SOXX', 'SPG', 'SPGI', 'SPIB', 'SPOT', 'SPXL', 'SRLN', 'SSO', 'STX', 'STZ', 'SVIX', 'SVXY', 'SWK', 'SYK', 'TDG', 'TEAM', 'TIP', 'TMO', 'TMUS', 'TPR', 'TRGP', 'TRV', 'TT', 'TTWO', 'TWLO', 'UAL', 'UL', 'ULTA', 'UNH', 'UNP', 'UPRO', 'UPS', 'UPST', 'URI', 'USFR', 'USHY', 'USO', 'UVIX', 'VB', 'VCIT', 'VCLT', 'VCSH', 'VGT', 'VIG', 'VLO', 'VMC', 'VO', 'VRSK', 'VRTX', 'VT', 'VTEB', 'VTIP', 'VTV', 'VTWO', 'VUG', 'VWO', 'WCN', 'WDAY', 'WDC', 'WEC', 'WELL', 'WING', 'WM', 'WMB', 'WSM', 'WYNN', 'XHB', 'XLC', 'XLRE', 'XOP', 'YUM', 'ZS', 'ZTS']\n",
      "\n",
      "‚ùå Only 148 tickers had full 390 bars every day.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_paths = [\n",
    "    \"./2023-03-20.csv.gz\",\n",
    "    \"./2023-03-21.csv.gz\",\n",
    "    \"./2024-03-20.csv.gz\",\n",
    "    \"./2025-03-20.csv.gz\",\n",
    "    # ... up to 20 days\n",
    "]\n",
    "\n",
    "verify_consistent_390_minute_tickers_with_filter(file_paths, ticker_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717e2ae6-752e-44bf-8271-a3561113d11a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
